{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression vs LogRegCCD on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./notebooks\"):\n",
    "    %cd ..\n",
    "    %cd ..\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, ConfusionMatrixDisplay\n",
    "from src.log_reg_ccd import LogRegCCD\n",
    "from src.data.data_loader import SyntheticDataLoader\n",
    "from src.data.dataset_interface import DataInterface\n",
    "import src.measures as measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User specified variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "chosen_experiment = 'Default' # Chosen experiment title - check under Experiments params section\n",
    "run_analyses = True # wheter to run parameters analyses\n",
    "results_dir = 'results' # If not None - directory to save te analyses plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentParams:\n",
    "    title: str\n",
    "    n: int\n",
    "    p: float\n",
    "    d: int\n",
    "    g: float\n",
    "    eps: float = 1e-3\n",
    "    lam_max: float = 10.0\n",
    "    lam_count: int = 100\n",
    "    k_fold: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_N = 1000\n",
    "DEFAULT_P = 0.5\n",
    "DEFAULT_D = 50\n",
    "DEFAULT_G = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucroc = measure.AUCROC()\n",
    "ba = measure.BalancedAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    \n",
    "    ExperimentParams(title='Default', n=DEFAULT_N, p=DEFAULT_P, d=DEFAULT_D, g=DEFAULT_G),\n",
    "\n",
    "    ExperimentParams(title='Few samples', n=100, p=DEFAULT_P, d=DEFAULT_D, g=DEFAULT_G),\n",
    "    ExperimentParams(title='Lots of samples', n=10000, p=DEFAULT_P, d=DEFAULT_D, g=DEFAULT_G),\n",
    "\n",
    "    ExperimentParams(title='Slightly imbalanced', n=DEFAULT_N, p=0.3, d=DEFAULT_D, g=DEFAULT_G),\n",
    "    ExperimentParams(title='Highly impalanced', n=DEFAULT_N, p=0.1, d=DEFAULT_D, g=DEFAULT_G),\n",
    "\n",
    "    ExperimentParams(title='Few features', n=DEFAULT_N, p=DEFAULT_P, d=10, g=DEFAULT_G),\n",
    "    ExperimentParams(title='Lots of features', n=DEFAULT_N, p=DEFAULT_P, d=500, g=DEFAULT_G),\n",
    "    ExperimentParams(title='More features than samples', n=DEFAULT_N, p=DEFAULT_P, d=1500, g=DEFAULT_G),\n",
    "\n",
    "    ExperimentParams(title='Weak correlation', n=DEFAULT_N, p=DEFAULT_P, d=DEFAULT_D, g=0.1),\n",
    "    ExperimentParams(title='Moderate correlation', n=DEFAULT_N, p=DEFAULT_P, d=DEFAULT_D, g=0.5),\n",
    "    ExperimentParams(title='Strong correlation', n=DEFAULT_N, p=DEFAULT_P, d=DEFAULT_D, g=0.8),\n",
    "\n",
    "    ExperimentParams(title='Moderate correlation, lots of features', n=DEFAULT_N, p=DEFAULT_P, d=500, g=0.5),\n",
    "    ExperimentParams(title='Strong correlation, lots of features', n=DEFAULT_N, p=DEFAULT_P, d=500, g=0.8),\n",
    "]\n",
    "\n",
    "experiment = next(exp for exp in experiments if exp.title == chosen_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X, y, save_path=None, ccd=True, pr=True):\n",
    "    \"\"\" Evaluate model according to chosen metrics\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    if ccd:\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "        results['coefficients'] = model.best_beta\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X)[:,1]\n",
    "        results['coefficients'] = np.concatenate((model.intercept_, model.coef_[0]))\n",
    "\n",
    "    results['accuracy'] = accuracy_score(y, y_pred)\n",
    "    results['auc_roc'] = aucroc(y, y_pred_proba)\n",
    "    results['balanced_accuracy'] = ba(y, y_pred)\n",
    "\n",
    "    if pr:\n",
    "        print(f\"\\nCoefficients: {results['coefficients']}\")\n",
    "        print(f\"\\n{color.CYAN}{color.UNDERLINE}{aucroc}{color.END}: {results['auc_roc']}\")\n",
    "        print(f\"\\n{color.CYAN}{color.UNDERLINE}{ba}{color.END}: {results['balanced_accuracy']}\")\n",
    "        print(f\"\\nAccuracy: {results['accuracy']:.4f}\")\n",
    "\n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            f.write(\"=\"*50 + \"\\n\")\n",
    "            f.write(\"MODEL EVALUATION RESULTS\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"COEFFICIENTS:\\n\")\n",
    "            f.write(str(results['coefficients']) + \"\\n\\n\")\n",
    "            \n",
    "            if 'auc_roc' in results:\n",
    "                f.write(f\"AUC-ROC SCORE: {results['auc_roc']:.4f}\\n\\n\")\n",
    "            \n",
    "            if 'balanced_accuracy' in results:\n",
    "                f.write(f\"BALANCED ACCURACY: {results['balanced_accuracy']:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"ACCURACY: {results['accuracy']:.4f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        plot_path = str(save_path).replace('.txt', '_confusion_matrix.pdf')\n",
    "        plt.savefig(plot_path, format='pdf', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex(experiment):\n",
    "    \"\"\"Method to run single experiment.\"\"\"\n",
    "\n",
    "    di = DataInterface(SyntheticDataLoader(experiment.p, experiment.n, experiment.d, experiment.g, random_seed))\n",
    "    di.split_data(val_size=0.2, test_size=0.3)\n",
    "    data = di.get_data()\n",
    "    X_train, y_train = data['train_data'], data['train_labels']\n",
    "    X_valid, y_valid = data['val_data'], data['val_labels']\n",
    "    X_test, y_test = data['test_data'], data['test_labels']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(penalty=None)\n",
    "    model.fit(X_train, y_train)\n",
    "    results_lr = test_model(model, X_test, y_test, ccd=False, pr=False)\n",
    "\n",
    "    model_ccd = LogRegCCD(verbose=False)\n",
    "    model_ccd.fit(X_train, y_train, experiment.eps, experiment.lam_max, experiment.lam_count, experiment.k_fold)\n",
    "    results_ccd = test_model(model_ccd, X_test, y_test, pr=False)\n",
    "\n",
    "    model_ccd.validate(X_valid, y_valid, measure=aucroc)\n",
    "    result_ccd_auc_roc = test_model(model_ccd, X_test, y_test, pr=False)\n",
    "\n",
    "    model_ccd.validate(X_valid, y_valid, measure=ba)\n",
    "    results_ccd_ba = test_model(model_ccd, X_test, y_test, pr=False)\n",
    "\n",
    "    return {\n",
    "        'results_lr': results_lr,\n",
    "        'results_ccd': results_ccd,\n",
    "        'result_ccd_auc_roc': result_ccd_auc_roc,\n",
    "        'results_ccd_ba': results_ccd_ba\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_variable_experiments(variable_name, variable_values):\n",
    "    \"\"\"Method to run anylysis regarding chosen variable.\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for value in variable_values:\n",
    "        if variable_name == 'n':\n",
    "            exp = ExperimentParams(title=f'n_{value}', n=value, p=DEFAULT_P, d=DEFAULT_D, g=DEFAULT_G)\n",
    "        elif variable_name == 'p':\n",
    "            exp = ExperimentParams(title=f'p_{value}', n=DEFAULT_N, p=value, d=DEFAULT_D, g=DEFAULT_G)\n",
    "        elif variable_name == 'd':\n",
    "            exp = ExperimentParams(title=f'd_{value}', n=DEFAULT_N, p=DEFAULT_P, d=value, g=DEFAULT_G)\n",
    "        elif variable_name == 'g':\n",
    "            exp = ExperimentParams(title=f'g_{value}', n=DEFAULT_N, p=DEFAULT_P, d=DEFAULT_D, g=value)\n",
    "        \n",
    "        ex_result = ex(exp)\n",
    "        results.append({\n",
    "            'variable': variable_name,\n",
    "            'value': value,\n",
    "            'results_lr': ex_result['results_lr'],\n",
    "            'results_ccd': ex_result['results_ccd'],\n",
    "            'result_ccd_auc_roc': ex_result['result_ccd_auc_roc'],\n",
    "            'results_ccd_ba': ex_result['results_ccd_ba']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(variable_results, metric='auc_roc'):\n",
    "    \"\"\"Method to plot synthetic data parameters influence analysis\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    variables = ['n', 'p', 'd', 'g']\n",
    "    titles = ['Varying n (samples)', 'Varying p (class balance)', 'Varying d (features)', 'Varying g (correlation)']\n",
    "    \n",
    "    for i, var in enumerate(variables):\n",
    "        ax = axes[i]\n",
    "        var_data = [r for r in variable_results if r['variable'] == var]\n",
    "        \n",
    "        x = [d['value'] for d in var_data]\n",
    "        y_lr = [d['results_lr'][metric] for d in var_data]\n",
    "        y_ccd = [d['results_ccd'][metric] for d in var_data]\n",
    "        y_auc_ccd = [d['result_ccd_auc_roc'][metric] for d in var_data]\n",
    "        y_ba_ccd = [d['results_ccd_ba'][metric] for d in var_data]\n",
    "\n",
    "        ax.plot(x, y_lr, label='Logistic Regression', marker='o')\n",
    "        ax.plot(x, y_ccd, label='CCD', marker='s')\n",
    "        ax.plot(x, y_auc_ccd, label='AUC ROC CCD', marker='s')\n",
    "        ax.plot(x, y_ba_ccd, label='BA CCD', marker='s')\n",
    "        ax.set_title(titles[i])\n",
    "        ax.set_xlabel(var)\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data parameters influence analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.linspace(100, 10000, num=2, dtype=int) \n",
    "p_values = np.linspace(0.1, 0.9, num=2)\n",
    "d_values = np.linspace(10, 500, num=2, dtype=int)\n",
    "g_values = np.linspace(0, 0.9, num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyses run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_analyses:\n",
    "    all_results = []\n",
    "    all_results.extend(run_variable_experiments('n', n_values))\n",
    "    all_results.extend(run_variable_experiments('p', p_values))\n",
    "    all_results.extend(run_variable_experiments('d', d_values))\n",
    "    all_results.extend(run_variable_experiments('g', g_values))\n",
    "\n",
    "    fig_auc = plot_results(all_results, metric='auc_roc')\n",
    "    fig_ba = plot_results(all_results, metric='balanced_accuracy')\n",
    "\n",
    "    if results_dir:\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        fig_auc.savefig(os.path.join(results_dir, 'auc_roc_comparison.png'), bbox_inches='tight')\n",
    "        fig_ba.savefig(os.path.join(results_dir, 'balanced_accuracy_comparison.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosen experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = DataInterface(SyntheticDataLoader(experiment.p, experiment.n, experiment.d, experiment.g, random_seed))\n",
    "di.split_data(val_size=0.2, test_size=0.3)\n",
    "data = di.get_data()\n",
    "X_train, y_train = data['train_data'], data['train_labels']\n",
    "X_valid, y_valid = data['val_data'], data['val_labels']\n",
    "X_test, y_test = data['test_data'], data['test_labels']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=None)\n",
    "model.fit(X_train, y_train)\n",
    "_ = test_model(model, X_test, y_test, ccd=False, pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogRegCCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ccd = LogRegCCD(verbose=False)\n",
    "model_ccd.fit(X_train, y_train, experiment.eps, experiment.lam_max, experiment.lam_count, experiment.k_fold)\n",
    "_ = test_model(model_ccd, X_test, y_test, pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ccd.plot(X_valid, y_valid, measure=aucroc)\n",
    "_ = model_ccd.validate(X_valid, y_valid, measure=aucroc)\n",
    "_ = test_model(model_ccd, X_test, y_test, pr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ccd.plot(X_valid, y_valid, measure=ba)\n",
    "_ = model_ccd.validate(X_valid, y_valid, measure=ba)\n",
    "_ = test_model(model_ccd, X_test, y_test, pr=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
